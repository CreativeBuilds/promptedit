# -*- coding: utf-8 -*-
"""promptedit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytIzG92vf3chHcUVffSZdwQm7aTGsaAT
"""

!nvcc --version

"""# install packages"""

!pip install -q -U scipy
!pip install -U -q git+https://github.com/huggingface/transformers@de9255de27abfcae4a1f816b904915f0b1e23cd9
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/timdettmers/bitsandbytes.git
# %cd bitsandbytes
!CUDA_VERSION=118 make cuda11x
!python setup.py install

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

"""# Load tokenizer and the model"""

model = AutoModelForCausalLM.from_pretrained(
    'cerebras/btlm-3b-8k-base',
    load_in_4bit=True,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map='auto'
)

# check we are in 4bit
model.transformer.h[3].attn.c_attn

tokenizer = AutoTokenizer.from_pretrained("cerebras/btlm-3b-8k-base")

"""# Text prompting

## Tokenize prompt
"""

# Set the prompt for generating text
prompt = "Albert Einstein was known for "

# Tokenize the prompt and convert to PyTorch tensors
inputs = tokenizer(prompt, return_tensors="pt")

"""## Generate response"""

# Generate text using the model
outputs = model.generate(
    **inputs,
    num_beams=5,
    max_new_tokens=50,
    early_stopping=True,
    no_repeat_ngram_size=2
)

# Convert the generated token IDs back to text
generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)

# Print the generated text
print(generated_text[0])